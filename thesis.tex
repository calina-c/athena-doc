%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%
%%% File: thesis.tex, version 1.9, May 2015
%%%
%%% =============================================
%%% This file contains a template that can be used with the package
%%% cs.sty and LaTeX2e to produce a thesis that meets the requirements
%%% of the Computer Science Department from the Technical University of Cluj-Napoca
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[12pt,a4paper,twoside]{report}         
\usepackage{cs}              
\usepackage{times}
\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{amsmath,amsbsy}
\usepackage{amssymb}
\usepackage[matrix,arrow]{xy}
\usepackage[T1]{fontenc}
\usepackage{ae,aecompl}
%\usepackage{shortcut} %definitii pentru diacritice; 
\usepackage{amstext}
\usepackage{graphics}
\usepackage[T1]{fontenc}
\usepackage{ae,aecompl}
\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage{color}
\usepackage{color}
\usepackage{url}
\usepackage{listings}

\mastersthesis
%\diplomathesis
% \leftchapter
\centerchapter
% \rightchapter
\singlespace
% \oneandhalfspace
% \doublespace

\renewcommand{\thesisauthor}{Ioana-C\u{a}lina TUTUNARU}    %% Your name.
\renewcommand{\thesismonth}{July}     %% Your month of graduation.
\renewcommand{\thesisyear}{2016}      %% Your year of graduation.
\renewcommand{\thesistitle}{ATHENA - Aproach for Twitter Harvesting, Enhancement, Normalisation \& Analysis} 
\renewcommand{\thesissupervisor}{prof. dr. ing. Ioan Salomie}
\newcommand{\department}{\bf FACULTY OF AUTOMATION AND COMPUTER SCIENCE\\
COMPUTER SCIENCE DEPARTMENT}
\newcommand{\thesis}{MASTER THESIS}
\newcommand{\utcnlogo}{\includegraphics[width=15cm]{img/tucn.jpg}}

\newcommand{\uline}[1]{\rule[0pt]{#1}{0.4pt}}
%\renewcommand{\thesisdedication}{P\u{a}rin\c{t}ilor mei}

\begin{document}
%\frontmatter
%\pagestyle{headings}

\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}



%\thesistitle                    %% Generate the title page.
%\authordeclarationpage                %% Generate the declaration page.

\pagenumbering{arabic}
\setcounter{page}{4}



\begin{center}
\utcnlogo

\department

\vspace{4cm}

{\bf \thesistitle} %LICENSE THESIS TITLE}

\vspace{1.5cm}

MASTER THESIS

\vspace{5cm}

Graduate: {\bf Ioana-C\u{a}lina TUTUNARU} 

Supervisor: {\bf \thesissupervisor}

\vspace{3cm}
{\bf \thesisyear}
\end{center}

\thispagestyle{empty}
\newpage

\begin{center}
\utcnlogo

\department

\end{center}
\vspace{0.5cm}

%\begin{small}
\begin{tabular}{p{7cm}p{8cm}}
 %\hspace{-1cm}& APPROVED,\\
 \hspace{-1cm}DEAN, & HEAD OF DEPARTMENT,\\
\hspace{-1cm}{\bf Prof. dr. eng. Liviu MICLEA} & {\bf Prof. dr. eng. Rodica POTOLEA}\\  
\end{tabular}
 
\vspace{1cm}

\begin{center}
Graduate: {\bf \thesisauthor}

\vspace{0.5cm}

{\bf \thesistitle}
\end{center}

\begin{enumerate}
 \item {\bf Project proposal:} {\it This approach combines time-proven unsupervised algorithms and specialised, state of the art text feature extraction with components of user-oriented business systems, to provide a solid but friendly decision support system of social media analysis. In essence, I present a process of data extraction that simplifies the task of analysing social media feeds, for business users and other stakeholders with little or no expertise in the domain of text feature extraction..}
\item {\bf Project contents:} {\it (Project Objectives and Specifications, Bibliographic research, Analyisis and Theoretical Foundation, Detailed Design and Implementa- tion, Testing and validation, Userâ€™s manual, Conclusions, Bibliography, Abbreviations and shorthand, Relevant code}
\item {\bf Place of documentation:} {\it Technical University of Cluj-Napoca, Computer Science Department}
\item {\bf Consultants:} Prof. dr. ing. Ioan SALOMIE
\item {\bf Date of issue of the proposal:} November 25, 2015
\item {\bf Date of  delivery:} July 5, 2016 {\it (the date when the document is submitted)}
  \end{enumerate}
\vspace{1.2cm}

\hspace{6cm} Graduate: \uline{6cm} 

\vspace{0.5cm}
\hspace{6cm} Supervisor: \uline{6cm} 
%\end{small}

\thispagestyle{empty}


\newpage
$ $
%\begin{center}
%\utcnlogo

%\department
%\end{center}

\thispagestyle{empty}
\newpage

\begin{center}
\utcnlogo

\department
\end{center}

\begin{center}
{\bf
Declara\c{t}ie pe proprie r\u{a}spundere privind\\ 
autenticitatea lucr\u{a}rii de licen\c{t}\u{a}}
\end{center}
\vspace{0.5cm}



Subsemnatul(a) \\
\uline{14.8cm}, 
legitimat(\u{a}) cu \uline{4cm} seria \uline{3cm} nr. \uline{4cm}\\
CNP \uline{9cm}, autorul lucr\u{a}rii \uline{2.8cm}\\
\uline{16cm}\\
\uline{16cm}\\
elaborat\u{a} \^{\i}n vederea sus\c{t}inerii examenului de finalizare a studiilor de masterat la Facultatea de Automatic\u{a} \c{s}i Calculatoare, Specializarea \uline{7cm} din cadrul Universit\u{a}\c{t}ii Tehnice din Cluj-Napoca, sesiunea \uline{4cm} a anului universitar \uline{3cm}, declar pe proprie r\u{a}spundere, c\u{a} aceast\u{a} lucrare este rezultatul propriei activit\u{a}\c{t}i intelectuale, pe baza cercet\u{a}rilor mele \c{s}i pe baza informa\c{t}iilor ob\c{t}inute din surse care au fost citate, \^{\i}n textul lucr\u{a}rii \c{s}i \^{\i}n bibliografie.

Declar, c\u{a} aceast\u{a} lucrare nu con\c{t}ine por\c{t}iuni plagiate, iar sursele bibliografice au fost folosite cu 
respectarea legisla\c{t}iei rom\^{a}ne \c{s}i a conven\c{t}iilor interna\c{t}ionale privind drepturile de autor.

Declar, de asemenea, c\u{a} aceast\u{a} lucrare nu a mai fost prezentat\u{a} \^{\i}n fa\c{t}a unei alte comisii de examen de licen\c{t}\u{a}.

\^{I}n cazul constat\u{a}rii ulterioare a unor declara\c{t}ii false, voi suporta sanc\c{t}iunile administrative, respectiv, \emph{anularea examenului de licen\c{t}\u{a}}.

\vspace{1.5cm}

Data \hspace{8cm} Nume, Prenume

\vspace{0.5cm}

\uline{3cm} \hspace{5cm} \uline{5cm}

\vspace{1cm}
\hspace{9.4cm}Semn\u{a}tura

\thispagestyle{empty}

\thispagestyle{empty}
\newpage

\begin{center}
\utcnlogo

\department
\end{center}

\begin{center}
{\bf
Cerere de elaborare a diserta\c{t}iei \^{\i}n limba Englez\u{a}}
\end{center}
\vspace{0.5cm}

Subsemnatul(a) \\
\uline{14.8cm}, 
legitimat(\u{a}) cu \uline{4cm} seria \uline{3cm} nr. \uline{4cm}\\
CNP \uline{9cm}, autorul lucr\u{a}rii \uline{2.8cm}\\
\uline{16cm}\\
\uline{16cm}\\
elaborat\u{a} \^{\i}n vederea sus\c{t}inerii examenului de finalizare a studiilor de masterat la Facultatea de Automatic\u{a} \c{s}i Calculatoare, Specializarea \uline{7cm} din cadrul Universit\u{a}\c{t}ii Tehnice din Cluj-Napoca, sesiunea \uline{4cm} a anului universitar \uline{3cm}, v\u{a} rog s\u{a}-mi aproba\c{t}i elaborarea lucr\u{a}rii de diserta\c{t}ie \^{\i}n limba Englez\u{a}.

Limba Englez\u{a} este de mare circula\c{t}ie interna\c{t}ional\u{a} \c{s}i prezint\u{a} bune oportunit\u{a}\c{t}i de discu\c{t}ie \^{\i}n comunitatea stiin\c{t}ific\u{a}. Doresc s\u{a} elaborez prezenta lucrare \^{\i}n limba Englez\u{a} pentru mai buna ei vizibilitate.

\vspace{0.5cm}

Data \hspace{8cm} Nume, Prenume

\vspace{0.5cm}

\uline{3cm} \hspace{5cm} \uline{5cm}

\vspace{0.5cm}
\hspace{9.4cm}Semn\u{a}tura

\vspace{0.5cm}
\hspace{8cm} \uline{5cm}

\vspace{0.5cm}
\textbf{Semn\u{a}turi aprobare:}

Profesor \^{\i}ndrumator: Prof. dr. Ioan SALOMIE: \uline{5cm}

\vspace{0.5cm}

\c{S}ef de catedra: Prof. dr. Rodica POTOLEA: \uline{5cm}

\vspace{0.5cm}

Decan: Prof. dr. Liviu MICLEA: \uline{5cm}

\newpage


%\listoftables
%\listoffigures

%\clearpage 
%\newpage

\newpage

\listoffigures
\tableofcontents
\newpage



\include{intro}
\include{specs}
\include{bibresearch}
\include{analysis}
\include{ch5}

\chapter{Testing and Validation}
An important issue with developing novel approaches and application implementations for text feature extraction is validating the results. The endeavour of fully and completely testing such a domain-independent approach is, however, technically impossible. This is because we lack any adnotated data that could span over virtually all possible domains of application, but also due to the intrinsic nature of the algorithms themselves. Therefore, in the following section, I will handle two threads of discussion: proving the corectness of components used throughout the application, and empyrical testing of the application results, in different domains.

\section{Component testing}
ATHENA, both as approach and as implementation, is heavily reliant on the SOLID principles. One of these, Dependency Inversion, stating that components must always depend on more stable libraries and components, is key to the results obtained.

\subsection{The Django framework}
The Django framework encourage developers to aim for a high testing code coverage. It supports this general activity by supplying unit tests of its own to components and overall architecture. Django writes the tests in-house and with the help of the open-source community, having covered important bases in functionalities we use such as Request/Response handling and Generic Views. Since the web application is only loosely coupled with the underlying algorithmic implementation, further web tests are not necessary when using these proven tools.

\subsection{Algorithms}
On the other hand, the algorithms which compose the main functionalities, such as vectorisation, power law fitting, clustering and the calculation of relevant numerical data are less covered to a lesser extent and fewer cases. Most of the times, they are theoretically and matematically proven, so the requirement for unit tests only applies to the implementation and not the method itself.

The SciKit modules for Vectorisation and KMeans Clustering are of course test-covered. The application of these methods into code has been thoroughly tested. The \texttt{plfit} module has also been tested and ships with unit tests and corresponding test fixtures. In the case of the latter, the approach is simply to generate the power functions with known parameters, running the fitting algorithm and comparing the results to the known examples.

\section{Empyrical testing of the application results}
Throughout the thesis the examples were based on the use case of programming language comparison and exemplified with the \texttt{\#python} and \texttt{\#php} hashtags. In the next section I will present more examples of text feature extraction results.

An important word of note about the examples that follow is that they will most likely not age well as examples. This is due mainly to a couple of factors: one is the restrictions imposed by the Twitter Search API, which refuses to return any tweets older than one week. Another issue to note is social media generally (and especially Twitter) are mostly used for trending topics and viral information. These facts make it difficult to follow classical or older topics, but work well for timeless, viral and/or trending hashtags.

\subsection{In recent movies}
In recent times, the comic book company Marvel has released it's own live action movies, with rising popularity for these productions. The last weeks have seen the release of "Deadpool" and "X-Men: Apocalypse". The movies have been highly covered by reviewers, bloggers and social media users. Out of the three, "Deadpool" has been the most acclaimed, gaining a 84\% rating on Rotten Tomatoes and an 8.2 score on IMDB (as of July 2nd 2016).

An Enhancement run on the \texttt{\#deadpool} hashtag reveals related movies and concepts:

\texttt{\#bot \#comics \#contest \#cosplay \#deadpoolmovie \#funko \#funkopop \#gameofthrones \#giveaway \#hatsofftohrithik \#hottopic \#marketing \#marvel \#marvelcomics \#mystery \#mysterypop \#rightintheface \#rustom \#ryanreynolds \#socialmediaday \#spiderman \#starwars \#wolverine \#xmen}

Out of these, movies released in the same period or related by genre, representations of Deadpool (comics and movies), references to the parent company (Marvel) and/or other superhero movies.

Clusters reveal even more interesting ideas, with clusters generally corresponding to directions of the social media talks around deadpool.

\begin{itemize}
\item comics and comic book movies (\texttt{\#comics \#cosplay \#deadpool \#deadpoolmovie \#funko \#funkopop \#marvel \#spiderman \#xmen})
\item marketing and social promotions (\texttt{\#becausescience \#bot \#comics \#deadpool \#deadpoolmovie \#marketing \#rightintheface \#sdcc \#socialmediaday \#wolverine})
\item related movies and shows (\texttt{\#art \#batman \#becausescience \#bot \#comedy \#comics \#deadpool \#gameofthrones \#wolverine \#xmen})
\item contests and giveaways (\texttt{\#comedy \#comics \#contest \#deadpool \#funko \#funkopop \#giveaway \#hottopic \#mystery \#mysterypop})
\item related cinematic releases(\texttt{\#2} as in the eventually upcoming Deadpool 2, \texttt{\#art \#comedy \#comics \#cosplay \#deadpool \#marvel \#marvelcomics \#ryanreynolds \#starwars})
\end{itemize}

Between the normalised harvests based on \texttt{\#deadpool} and \texttt{\#xmenapocalypse}, it is no surprise the common vocabulary consists of two hashtags: \texttt{\#marvel} and \texttt{\#xmen}, indeed with "Deadpool" taking place in the same cinematic universe as the X-Men series, with cameos from two of the the X-Men characters. However, as the breakdown suggests, the movie was standalone, and there are not many posts about both movies, partly also because they were released with a couple of weeks distance inbetween.

On analysing users, however, we find that fans of comic book movies are mostly the same, with a number of 19 common users. The revelation is indeed in the post breakdown: 71\% of users have recently posted about the "Deadpool" movie (even considering "X-Men:Apocalypse" was launched more recently and should be fresher and trending), as opposed to X-Men's 27\%. This only confirms "Deadpool"'s well-received originality and high ratings.

The case is mostly the same with a comparison between "Deadpool" and DC's "Batman vs. Superman: Dawn of Justice". The difference here is that, rather than comparing two movies released by the same company at different times, the comparison is between movies released by competing companies at more or less the same time. Rotten Tomatoes and IMDB scores for the "Batman vs. Superman" movies were generally low, with the comic book fanbase being generally disappointed with the movie. 

\subsection{In current politics}
In testing the current status of popular tweets, politics is not a domain to be ignored. The following subsections describes ATHENA's results on processing tweets about the current USA Presidential Campaign, with candidates Hillary Clinton and Donald Trump using Twitter as a political advertisment platform. However, various articles and news outlets have stated that Trump's so-called "Twitter game" obviously attracts more discussion than his counter candidate's\footnote{http://www.slate.com/articles/technology/future\_tense/2016/02/

donald\_trump\_is\_the\_best\_at\_twitter\_here\_s\_why.html}. Indeed the popularity of discussions isn't necessarily related to the outcome of the vote, with discussion generally falling into positive or negative categories. However, the phenomenon is interesting to be studied.

A one-day Harvest of the \texttt{\#trump} hashtag provides a number of 22,301 posts by 13,366 users, with a maximum posts per user of 128 (highly indicative of page or bot activity), well-fitted to a power function. This is to be expected of course, since the political campaign is in full fledge. Hashtag clusters organise as follows:

\begin{itemize}
\item media outlets and catchphrases related to the campaign (\texttt{\#dtmag \#2a \#trump \#trump2016 \#trumptrain \#imwithyou \#maga \#makeamericagreatagain \#growinguphispanic \#neverhillary})
\item recent events which are featured in his campaign: cluster 1 (\texttt{\#fullmonty \#garbage \#gaysfort \#gaysfortrump \#georgia \#globalism \#gop \#worldchanges \#hillary2}) 
\item campaign ideals (\texttt{\#fraud \#freedom \#freethedelegates \#fullmonty \#garbage \#gaysfort \#trump \#georgia \#globalism \#gaysfortrump \#makeamericagreatagain})
\item confrontational issues (\texttt{\#obama \#trump \#trump2016 \#trumptrain \#crookedhillary \#establishment \#canada \#populism \#hannity})
\item political associations (\texttt{\#trump \#borisjohnson \#uk \#brexit \#imwithher \#democracy \#sanders \#scotland \#boris \#fraud})\footnote{most of these hashtags refer to the United Kingdom's vote to leave the European Union, with politicians such as Boris Johnson encouraging it. Users see the same discourse in the "Leave" politicians as in Trump's "Make America great again" anti-imigration policies.}
\end{itemize}

The theory that Trump is prevalent on Twitter as opposed to Hillary Clinton is also proven to be quite close to the actual truth. From a normalisation of one-day harvests with the same start and end dates, using the hashtags \texttt{\#trump} and \texttt{\#hillary} reveal some interesting facts. First, the common vocabulary consisting of:

\texttt{\#maga \#hillaryclinton \#crookedhillary \#election2016 \#neverhillary \#tcot \#imwithher \#trump2016 \#bernie \#clinton \#obama}

These hashtags indicate that common posts about the two are often neutral or negative to Hillary Clinton. The number of common users is very large, with the 1,963 common tweets authored by 1,963 different users. This 1:1 proportion indicates that the users are mostly real accounts and that the candidates supporting pages avoid mentioning the other candidate's official hashtag(s), preferring to opt for negative ones, such as the previously mentioned \texttt{\#crookedhillary, \#neverhillary} etc.

Proportions and breakdown also speak volumes. The prevalence of Trump's online presence is proven, with 11,728 posts (71.4\% of the total breakdown) containing the \texttt{\#trump} hashtag and no mention of \texttt{\#hillary}. HIllary Clinton's exclusive mentions and the common mentions the candidates share are almost equivalent, with 2,737 (16.7\%) and the said 1,963 (11.9\%) respectively.

Results indeed confirm the theories that Twitter is dominated by Donald Trump's presence, albeit positive or negative.

\subsection{For related topics}
With related topics, we can generally expect to see large common vocabularies and sets of users, as well as more common posts inside the post breakdown.  An analysis run on the popular programming languages PHP (generally for backend development) and JavaScript (clasically for frontend development, but has recently spread to backend as well) reveals the following common hashtags:

\texttt{\#angularjs \#reactjs \#java \#html5 \#css \#html \#jquery \#webdev}

AngularJS, ReactJS and JQuery are frontend Javascript languages and most likely used in projects with a PHP backend, in the context of this query. CSS and HTML being the main components of frontend systems, they naturally appear in the list, alongside the general occupation of web development. Besides a large common vocabulary, the two languages share 64 users which have posted about the two in the course of one day.

\subsection{Summary}
In this Chapter I presented the fundamentals on testing the web and algorithmic components of the ATHENA app, as well as the general approval of the scientific community for the conceptual algorithms used. I also discussed why a full testing is virtually impossible and presented a few examples from various topics, as resulted from the application.

\chapter{User's manual}
The following Chapter is a short description of the installation process needed to run the ATHENA app. The instructions are written for Mac OSX El Capitan, however most of the instructions work on other *nix systems as well, i.e. flavours of Linux including Ubuntu.

\subsection*{Prerequisites}
For the installation and running of this project, you will need:

\begin{itemize}
\item XCode, the Mac toolkit for software development. It is generally a prerequisite for installing various programmming languages.
\item Python (installed via the native package manager or using \texttt{brew}
\item Pip, Python's installer application
\item optional (but recommended) VirtualEnv, a wrapper to permit different Python and Django versions on the same host platform, without the need for virtual machines.
\end{itemize}

\subsection*{Getting the codebase}
The project's codebase can be fetched from the CD attached to this physical thesis or via GitHub. For the GitHub version, run the command:

\texttt{git clone https://github.com/calina-c/athena.git}

Either option you chose, you should now have in your directory of choice a file structure containing: 2 folders: \texttt{athena} and \texttt{athena\_app} and a few other files including:

\begin{itemize}
\item a \texttt{README.md} describing part of these installation steps
\item a hidden \texttt{.gitignore} file used by \texttt{git} in order to know which files to upload to the GitHub servers
\item  the \texttt{manage.py} file which acts as a Django command line utility
\item the \texttt{requirements.txt} file listing the third party libraries needed to be installed
\end{itemize}  and .

\subsection*{Installing dependencies}
Install \texttt{redis} and \texttt{cassandra} on your systems per the recommended installation for your Operating System. Run \texttt{redis-server} and \texttt{cassandra} in separate terminal tabs. They need to stay up for the time the application is used.

As previously stated, the list of Python dependencies needed is listed in \texttt{requirements.txt}. Considering the \texttt{pip} installer utility is installed on your system or virtualenv, run the following command:

\texttt{pip install -r requirements.txt}

This command should fetch and install any necessary third-party Python libraries and prompt you if any errors or incompatibilities exist, as well as suggest solutions in these cases.

\subsection*{Creating the database structure}
Considering your Cassandra server is already running, you can run the \texttt{cqlsh} command to enter the Cassandra console. From there, create the database structure by running the queries (also listed in README.md):

\begin{lstlisting}
CREATE TABLE tweet (   twitterId text,   user text,   content text,   date timestamp,   retweets int,   likes int, hashtags list<text>, history uuid, PRIMARY KEY (twitterId));

CREATE TABLE harvest ( uuid uuid, start_date timestamp, end_date timestamp, hashtag text, done boolean, PRIMARY KEY(uuid));

CREATE TABLE normal (uuid uuid, name text, content text, PRIMARY KEY(uuid));
\end{lstlisting}

\subsection*{Running the application}
To run the application locally, run the command:

\texttt{python manage.py runserver}

from the root folder of the application, where the \texttt{manage.py} file resides. Then, start your modern browser of choice (Chrome, Safari or Firefox) and use the localhost URL to access the web application:

\texttt{http://localhost:8000/app/}

Use the navbar on the left to select a pipeline step from the application. If you need asynchronous jobs, start another process in a separate terminal for Celery, using:

\texttt{celery -A athena worker -l info}

The Celery worker also needs to run in the background constantly, in order to properly be able to function as a Consumer for asynchronous jobs, as described in previous Chapters.

\chapter{Conclusions}
The following chapter presents the results of the ATHENA approach for automatic and domain-independent text feature extraction, as applied to Social Media posts. The first section presents current achievements, while the second lists further work which could improve the product application, as well as the approach, generally.

\section{Achievements and results}
The task of combining proven methods and algorithms for text feature extraction and present them as a user friendly web application is not an easy one. Careful consideration was needed in scaffolding the architecture as a pipeline and handling asynchronous jobs to fetch external data using a Producer/Consumer behaviour.

\subsection{Application structure and usability}
The presentation of a highly-specialised text feature extraction system as a web application is advisable for the sheer familiarity of users. The components chosen were based on the popular Bootstrap font-end elements, which have been in development for a few years and aim to provide developers with easy to use CSS and JS snippets and visuals. The prevalence of Bootstrap-based websites is due to the popularity of this set of libraries and consists in advantage in presenting the users with a familiar visual and interaction experience.

The pipeline steps (Harvest, Enhance, Normalise, Analyse) are abstracted separately to the user, in order to keep the application uncluttered and for the purpose of separation of concerns. The presence of asynchronous jobs (as further explained) has benefits in the general User Experience as well, with users being able to submit jobs to the Harvest module and keep using any other modules until their jobs are finished.

There are as few forms and complications necessary, data is presented in a highly visual and engaging fashion. All these efforts have been made with the aim of simplifying these applications for non-expert users looking to analyse social media posts, without the need for them to be statistics experts.

\subsection{Architectural accomplishments}
With pipelining, one of the most important things to note is that the approach is easily extensible for adding newer modules and/or data sources, with the Harvesting module being robust and independent from the rest of the application. Handling API Rate Limits imposed by social media networks was also an interesting task, with its asynchronous sleeping being a good abstraction: the user is not aware of the job's internal workings and doesn't have to stop using the application until their data is fully imported.

As part of the Enhancement step, the data is fed into a few unsupervised algorithms which return useful information about the contents of the Harvest. One of these is clustering, which loosely organises an d groups social media hashtags as per their usage throughout the document collection. From the users' perspective, it is clear that they can visualise and grasp some concepts better, e.g. the general points of discussion, associated domain data, directions of development of certain topics etc. The Enhancement step is also important in detection of outlier users, with an application of this being able to discern between real, individual, personal users on one hand, and fake accounts, pages and bots on the other.

The Normalisation step flattens the data without bothering the users with much details. At the moment, this step contains a single algorithm to elliminate outliers, and presents it as a dropdown choice. This algorithm is in fact a variant of the HashMap duplication removal algorithm running in $O(n)$ complexity, as part of the general concern to process data rapidly and present results to the user as soon as possible.

While the Analysis step is not architecturally complex, reusing many of the algorithms and functions from Enhancement and having numerical calculations on top, it does present the user with some interesting insights. This step compares the structure and popularity of two Harvests which have gone through the Normalisation step. The user can have a glimpse into the general domain's structure and polarisation of users between the two input labels harvested. Features such as common vocabulary, common users and post number breakdown help analyse the domain and support the end user's decision to appeal to one social group or the other.

\section{Work in progress}
The possibilities for extending the current approach are virtually endless. There are a lot of unsupervised algorithms which can provide insight into the post and user structure of a given Harvest. For example, an interesting extension would be running the documents through a sentiment analysis module. However, it is beyond the scope of this first proof of concept to delve into particular analysis fields, but the reader should note that the effort of adding new functionalities has a minimal integration overhead, with the choice of Python as programming language permitting the usage of many third party libraries.

In quantitative measures, more backends could be added to the Harvesting module, with applications such as Facebook and Instagram being some of the most obvious candidates. Adding more algorithms and data processing options to Enhancement and Analysis are also easily-implemented quantitative extensions.

Another point of concern in regards to further developments is adding more options to the Normalisation step. Indeed as presented in previous chapters the current existence of a single flattening and filtering algorithm is meant as a first step towards developing more exciting new types of normalised Harvests.

The project is openly available on the GitHub open source platform, as describe in the previous installation chapter and software engineers concerned with unsupervised text feature extraction from social media sources are more than welcome to contribute new features.

\subsection*{Summary}
To sum up, ATHENA is a good approach for text feature extraction which combines specialised algorithms with a comfortable, user-friendly web application interface. The main goal was to discuss and prove the benefits of the approach itself, but also to develop a proof-of-concept application which handles basic analysis and sets up the general architecture. Despite challenges such as a large number of possible implementation choices, feature inclusion and domain-independence, ATHENA as general approach and ATHENA as a minimal application are successful endeavours into bringing text extraction software closer to non-expert business users.

%\addcontentsline {toc}{chapter}{Bibliography} 
\bibliographystyle{IEEEtran} 
\bibliography{thesis}%same file name as for .bib

\include{apendix}

\end{document}
